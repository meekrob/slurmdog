---
title: "Usage reports"
author: "David C. King"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

##### libraries
library(magrittr)
library(ggplot2)
library(lubridate)
library(dplyr)

#### constants
GB_PER_CPU = 3.75 # this is an alpine-specific value

#### functions
format_size <- function(bytes) {
  units = c("B", "KB", "MB", "GB", "TB", "PB")
  exponent = floor(log2(bytes)/10)
  divider = 2^(10*exponent)
  output = sprintf("%.2f %s", bytes/divider, units[1+exponent]) 
  names(output) <- names(bytes)
  return(output)
}

seconds_to_timeformat <- function(seconds)
{   
    orig_names = names(seconds)
    days = as.integer(seconds/(3600*24))
    seconds_remaining = seconds %% (3600*24)
    hours = as.integer(seconds_remaining/3600)
    seconds_remaining = seconds_remaining %% 3600
    minutes = as.integer(seconds_remaining/60)
    seconds = as.integer(seconds_remaining %% 60)
    output = ifelse(days > 0, sprintf("%d-%02d:%02d:%02d", days, hours, minutes, seconds),
                              sprintf("%02d:%02d:%02d", hours, minutes, seconds))
    names(output) <- orig_names
    return(output)
}
    
to_time_obj = function(str) { # for Submit,Start,End data
  #as.POSIXct(str, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC") 
  ymd_hms(str, tz = "UTC")
}

time2seconds <- function(column) {
  unlist(
    lapply(
      strsplit(column, "[-:]"),
      time2seconds_time_lst
    )
  )
}

time2seconds_time_lst <- function(time_lst) {
  coef = c(1,60,60*60,60*60*24) 
  revved = rev(time_lst)
  x = as.integer(revved)
  X = sum(x * coef[1:length(x)])
  X
}

```

## Data

This results from something like:

`sacct -P -n -a --format JobID,User,Group,State,Cluster,AllocCPUS,REQMEM,TotalCPU,Elapsed,MaxRSS,ExitCode,NNodes,NTasks,JobName,Start,Submit,End $@`

Read into `parse_sacct.py` in order to aggregate resource data from various job steps.

```{r data}
nt_raw = read.table('allcsu_concat.tsv', header=T, sep="\t")
#nt_raw = read.table('erinlab_usage/erinlab_usage.tsv', header=T, sep="\t")
nt = nt_raw
nt %<>% mutate(
  CPU_Utilized_Seconds = time2seconds(CPU_Utilized),
  allocMemGB = AllocCPUS * GB_PER_CPU,
  core_walltime_seconds = time2seconds(core_walltime),
  submit_time=to_time_obj(Submit), 
  start_time=to_time_obj(Start), 
  wait_time=difftime(start_time, submit_time),
  end_time = to_time_obj(End),
  timespan = difftime(end_time,submit_time),
                          
                          
)
  
nt[grepl("CANCELLED by", nt$State), "State"] <- "CANCELLED"

n_acompile_jobs = nt %>% filter(grepl("acompile", JobNames)) %>% nrow()
n_zero_runtime_jobs = nt %>% filter(Elapsed_raw == 0) %>% nrow()
n_zero_memory_jobs = nt %>% filter(MaxRSS_Utilized_raw == 0) %>% nrow()
n_zero_CPU_jobs = nt %>% filter(CPU_Utilized_Seconds == 0) %>% nrow()
cat(sprintf("\t%d 'acompile' jobs (out of %d total)\n", n_acompile_jobs, nrow(nt)))
cat(sprintf("\t%d jobs with zero elapsed seconds (out of %d total)\n", n_zero_runtime_jobs,  nrow(nt)))
cat(sprintf("\t%d jobs with zero memory used (out of %d total)\n", n_zero_memory_jobs,  nrow(nt)))
cat(sprintf("\t%d jobs with zero CPU used (out of %d total)\n", n_zero_CPU_jobs,  nrow(nt)))

# filters
nt %<>% filter(!grepl("acompile", JobNames))
nt %<>% filter(Elapsed_raw > 0 | MaxRSS_Utilized_raw > 0 | CPU_Utilized_Seconds > 0)

# subsets
#nt %>% filter(grepl("bigfish", JobNames)) -> nt_cebigfish
nt %>% filter(grepl("jupyter", JobNames)) -> nt_jupyter
save_nt_jupyter = nt_jupyter # just temporary to get through edits via chatGPT

table(nt$State)
userjobs = table(nt$User)
users_w_enough_data = names(userjobs[userjobs >= 10])
nt %<>% filter(User %in% users_w_enough_data)
```

```{r summarize by user}
user_stats = nt %>% group_by(User) %>% summarize(total_cpu_secs = sum(CPU_Utilized_Seconds),
                                                 total_mem_raw = sum(MaxRSS_Utilized_raw),
                                                 num_jobs = n()
                                                 ) %>%
                                       mutate(total_cpu = seconds_to_timeformat(total_cpu_secs),
                                              total_mem = format_size(total_mem_raw))


```

## Including Plots

You can also embed plots, for example:

```{r jupyter, echo=FALSE}
# https://chatgpt.com/share/6824ddbb-f720-8010-a95b-3710a88fd9cf

DO_ONLY_JUPYTER=F

if(DO_ONLY_JUPYTER) # Jupyterlab jobs
{
  nt_jupyter <- save_nt_jupyter %>% 
  filter(State != "FAILED") %>% 
  mutate(Type = "Single Job", # for display
         MaxRSS_Utilized_raw_GB = MaxRSS_Utilized_raw / 2^30)
}else # all jobs
{
  nt_jupyter <- nt %>%
  filter(AllocCPUS < 48) %>%
  filter(State != "RUNNING") %>%
  mutate(Type = "Single Job", # for display
         MaxRSS_Utilized_raw_GB = MaxRSS_Utilized_raw / 2^30)
}
  
max_cpu <- max(nt_jupyter$AllocCPUS)
print(max_cpu)


# Create a new data frame for the shaded area across full x-range, limit y-range if there will be a lot of empty space
top_mem_used = max(nt_jupyter$MaxRSS_Utilized_raw_GB)
visible_ymax = min(top_mem_used*1.1, max_cpu*GB_PER_CPU)
AllocMemGB = (1:max_cpu) * GB_PER_CPU
alloc_mem_df <- data.frame(
  AllocCPUS = 1:max_cpu,
  AllocMemGB = ifelse(AllocMemGB < visible_ymax, AllocMemGB, visible_ymax), 
  Type = "Allocated Memory"
)

# plot labels
if (DO_ONLY_JUPYTER)
{
  label="Onishlab_users jobs (1 year), Jupyterlab, Memory vs CPU, across State"
  dotsize = 1
  savefunc <- function() {ggsave("mem_versus_CPU_by_State_jupyter.pdf", width=20)}

}else # do all
{
  label="Onishlab_users jobs (1 year), All Jobs, Memory vs CPU, across State"
  dotsize = 3
  savefunc <- function() {ggsave("mem_versus_CPU_by_State_alljobs.pdf", height=21, width=40)}
}

ggplot() + 
  # shaded rectangles
  geom_rect(data = alloc_mem_df, 
            aes(xmin = AllocCPUS - 0.4, 
                xmax = AllocCPUS + 0.4,
                ymin = 0, 
                ymax = AllocMemGB,
                fill = Type), alpha = 1
            ) +
  # usage for each job
  geom_jitter(data = nt_jupyter,
             aes(x = AllocCPUS, 
                 y = MaxRSS_Utilized_raw_GB, 
                 color = Type,
                 #width = .005,
                 
                 ), 
             position = position_jitterdodge(jitter.width=.4),
             size = dotsize
             ) + 
  facet_wrap(~User) + 
  scale_x_continuous(breaks= seq(1,max_cpu)) + 
  scale_y_continuous(breaks = 4 * seq(1,max_cpu, by=2)) +
  scale_fill_manual(values = c("Allocated Memory" = "lightblue")) +
  scale_color_manual(values = c("Single Job" = "black")) +
  labs(x = "Allocated CPUs", y = "Memory Utilized (GB)",
       fill = "", color = "") +
  theme_classic() +
  theme(
        plot.title = element_text(size = 24, hjust = 0.5),
        # axis font sizes
        axis.title.x = element_text(size = 18),   # X-axis title
        axis.title.y = element_text(size = 18),   # Y-axis title
        axis.text.x = element_text(size = 16),    # X-axis tick labels
        axis.text.y = element_text(size = 14) ,    # Y-axis tick labels
        # legend font size
        legend.text = element_text(size = 20, face = "bold"),
        # grid
        panel.grid.major.y = element_line(color = "gray80"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        strip.text = element_text(size = 18),
    
        legend.position = "right") + 
  ggtitle(label=label)

savefunc()

```
```{r ntask advantage}
nt_jupyter %>% filter(Elapsed_raw > 0) %>% group_by(State) %>% summarize(ntask_advantage_ave = mean(ntask_advantage))


nt_jupyter %>% filter(Elapsed_raw > 0) %>% 
               mutate(Elapsed_Seconds = Elapsed_raw) %>%
  ggplot(aes(x = AllocCPUS, y = CPU_Utilized_Seconds/Elapsed_Seconds)) + geom_jitter() + facet_wrap(~User) + ggtitle("Advantage of NTASKS", subtitle = "Higher is better")

# nt_jupyter %>% filter(Elapsed_raw > 0) %>% 
#   ggplot(aes(x = AllocCPUS, y = CPU_Efficiency)) + geom_jitter() + facet_wrap(~User) + ggtitle("CPU Efficiency", subtitle = "CPU_Utilized / core-walltime")
```

```{r}

```



Total compute

```{r pressure, echo=FALSE}

nt_cebigfish %>% filter(State != "FAILED") %>% 
  ggplot(aes(x=MaxRSS_Utilized_raw/(2^30), y=time2seconds(CPU_Utilized)/60, color=State)) + 
  geom_point() + 
  facet_wrap(~factor(AllocCPUS)) + 
  ggtitle(label="CEBigfish, CPU-utilized vs CPU, across State")

```
### array jobs

```{r array jobs}

nt_array_jobs = nt %>% filter(grepl("_", JobID))
nt_array_jobs %<>% separate_wider_delim(JobID, delim = "_", names=c("array_base_id", "array_id")) 
nt_array_jobs %<>% mutate(
                          submit_time=to_time_obj(Submit), 
                          start_time=to_time_obj(Start), 
                          wait_time=difftime(start_time, submit_time),
                          end_time = to_time_obj(End),
                          timespan = difftime(end_time,submit_time),
                          core_walltime_seconds = time2seconds(core_walltime)
                          )


summarize_state = function(x) {
  if (all(x == 'COMPLETED')) {
    return ("ALL_COMPLETED");
  }
  return ("NOT_ALL_COMPLETED");
  
}

nt_array_summary <- nt_array_jobs %>% 
  group_by(array_base_id) %>% 
  summarize(overall_start = min(start_time), 
            user = first(User), 
            njobs = n(),
            overall_end = max(end_time), 
            overall_wall = difftime(overall_end, overall_start),
            overall_span = difftime(overall_end, min(submit_time)),
            state = summarize_state(State),
            sum_elapsed_raw = sum(Elapsed_raw),
            average_wait = mean(wait_time),
            total_CPU_time = sum(time2seconds(CPU_Utilized)),
            effective_CPU_efficiency = total_CPU_time / (mean(AllocCPUS)*as.integer(overall_wall)),
            array_speed_up = sum_elapsed_raw /as.integer(overall_wall))

lte_five_min = nt_array_summary$overall_span<300
lte_five_min_array_ids <- nt_array_summary %>% filter(lte_five_min) %>% pull(array_base_id)
nt_array_jobs_lte_five <- nt_array_jobs %>% filter(array_base_id %in% lte_five_min_array_ids)

maxtime_array_ids <- function(time) {
  nt_array_summary %>% filter(overall_span < time) %>% pull(array_base_id)
}

nt_array_jobs %>% filter(array_base_id %in% maxtime_array_ids(600)) %>%
  head(n = 100) %>%
  #filter(array_base_id %in% c("9232034", "12125204", "12110242")) %>% 
  mutate(array_id_int = as.integer(array_id)) %>% 
  ggplot(aes(xmin=array_id_int - .4,
            xmax = array_id_int + .4,
            ymin = wait_time,
            ymax = timespan,
            fill = State)) + geom_rect() +
  facet_wrap(~array_base_id) +
  scale_y_continuous(breaks = 60 * seq(1,10)) +
  scale_x_continuous(breaks = 0:20)



nt_array_summary %>% filter(state == "ALL_COMPLETED") %>% View()


nt_array_jobs_completed <- nt_array_jobs %>% filter(State == "COMPLETED")

```